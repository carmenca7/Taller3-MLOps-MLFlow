# -*- coding: utf-8 -*-
"""Taller3_Punto1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DkOQCeBkM0F0EoYEqvQq37I3eTfzqXui

#Taller 3 Punto 1 - MLOps
##Carmen Carvajal Gutiérrez
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pycaret

from pycaret.classification import *
import pandas as pd

# Cargar los datos
file_path = '/content/data.csv'
data = pd.read_csv(file_path)

# Configuración del entorno de PyCaret
setup_pycaret = setup(
    data=data,               # El dataset cargado
    target='Bankrupt?',      # Columna objetivo (etiqueta)
    preprocess=True,         # Activar preprocesamiento automático
    session_id=42            # Para reproducibilidad
)

# Comparar modelos para obtener una visión general
top_3_models = compare_models(sort='AUC', n_select=3)

# Seleccionar los 3 mejores modelos
top_3_models

# Buscar hiperparámetros óptimos para los mejores modelos
tuned_models = [tune_model(model) for model in top_3_models]

# Mostrar los modelos ajustados
tuned_models

# DataFrame con los resultados de cada modelo ajustado
model_results = []
for model in tuned_models:
    results = pull()  # Extrae las métricas del modelo actual en PyCaret
    mean_auc = results.loc["Mean", "AUC"]  # Obtener el AUC promedio
    std_auc = results.loc["Std", "AUC"]   # Obtener la desviación estándar del AUC
    model_results.append({"Model": str(model), "Mean_AUC": mean_auc, "Std_AUC": std_auc})

# Convertir los resultados a un DataFrame
df_results = pd.DataFrame(model_results)

# Identificar el modelo con el mayor AUC promedio
best_model = df_results.loc[df_results['Mean_AUC'].idxmax()]

# Mostrar el mejor modelo
print(f"El mejor modelo es: {best_model['Model']}")
print(f"Con un AUC promedio de: {best_model['Mean_AUC']}")
print(f"Desviación estándar de AUC: {best_model['Std_AUC']}")

"""# El mejor modelo
El mejor modelo es el Light Gradient Boosting Machine, con un AUC promedio de 93.14% y una desviación estándar de 2.77%, según la métrica AUC, teniendo en cuenta que el dataset con el que se está trabajando esta muy desequilibrado. No se realizo el análisis con la métrica de F1-Score, como en el anterior taller, ya que los resultados son muy bajos.

##¿Difieren los resultados respecto a lo hecho con Scikit Learn en el taller pasado?
En el taller pasado, tanto los modelos como las métricas presentaron resultados significativamente buenos, con valores superiores al 90%. Sin embargo, los nuevos resultados muestran diferencias notables en términos de Recall, Precisión y F1-Score en todos los modelos evaluados (son muy bajos), lo que podría atribuirse a la metodología usada en esta ocasión.
"""